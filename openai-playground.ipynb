{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the book\n",
    "loader = PyPDFLoader(\"David-Copperfield.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cut out the open and closing parts\n",
    "pages = pages[6:1308]\n",
    "# Combine the pages, and replace the tabs with spaces\n",
    "text = ' '.join([page.page_content.replace('\\t', ' ') for page in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\x18 Free eBooks at Planet eBook.comsee you.’\\nMY mother was too much afraid of her to refuse compli -\\nance with this odd request, if she had any disposition to do \\nso. Therefore she did as she was told,'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "   # Remove the specific phrase 'Free eBooks at Planet eBook.com' and surrounding whitespace\n",
    "   cleaned_text = re.sub(r'\\s*Free eBooks at Planet eBook\\.com\\s*', '', text, flags=re.DOTALL)\n",
    "   # Remove extra spaces\n",
    "   cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
    "   # Remove non-printable characters, optionally preceded by 'David Copperfield'\n",
    "   cleaned_text = re.sub(r'(David Copperfield )?[\\x00-\\x1F]', '', cleaned_text)\n",
    "   # Replace newline characters with spaces\n",
    "   cleaned_text = cleaned_text.replace('\\n', ' ')\n",
    "   # Remove spaces around hyphens\n",
    "   cleaned_text = re.sub(r'\\s*-\\s*', '', cleaned_text)\n",
    "   return cleaned_text\n",
    "clean_text=clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 463999 tokens in the book\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "Tokens = llm.get_num_tokens(clean_text)\n",
    "print (f\"We have {Tokens} tokens in the book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "   OpenAIEmbeddings(), breakpoint_threshold_type=\"interquartile\"\n",
    ")\n",
    "docs = text_splitter.create_documents([clean_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "def get_embeddings(text):\n",
    "   response = openai.embeddings.create(\n",
    "       model=\"text-embedding-3-small\",\n",
    "       input=text\n",
    "   )\n",
    "   return response.data\n",
    "embeddings=get_embeddings([doc.page_content for doc in docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "content_list = [doc.page_content for doc in docs]\n",
    "df = pd.DataFrame(content_list, columns=['page_content'])\n",
    "vectors = [embedding.embedding for embedding in embeddings]\n",
    "array = np.array(vectors)\n",
    "embeddings_series = pd.Series(list(array))\n",
    "df['embeddings'] = embeddings_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "# Convert to float32 if not already\n",
    "array = array.astype('float32') \n",
    "num_clusters = 50\n",
    "# Vectors dimensionality\n",
    "dimension = array.shape[1] \n",
    "# Train KMeans with Faiss\n",
    "kmeans = faiss.Kmeans(dimension, num_clusters, niter=20, verbose=True)\n",
    "kmeans.train(array)\n",
    "# Directly access the centroids\n",
    "centroids = kmeans.centroids \n",
    "# Create a new index for the original dataset\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "# Add original dataset to the index\n",
    "index.add(array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(centroids, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_array = np.sort(I, axis=0)\n",
    "sorted_array=sorted_array.flatten()\n",
    "extracted_docs = [docs[i] for i in sorted_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(temperature=0,model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will be given different passages from a book one by one. Provide a summary of the following text. Your result must be detailed and atleast 2 paragraphs. When summarizing, directly dive into the narrative or descriptions from the text without using introductory phrases like 'In this passage'. Directly address the main events, characters, and themes, encapsulating the essence and significant details from the text in a flowing narrative. The goal is to present a unified view of the content, continuing the story seamlessly as if the passage naturally progresses into the summary.\n",
    "Passage:\n",
    "```{text}```\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= (\n",
    "    prompt\n",
    "   | model\n",
    "   |StrOutputParser() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 50/50 [04:19<00:00,  5.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "final_summary = \"\"\n",
    "\n",
    "for doc in tqdm(extracted_docs, desc=\"Processing documents\"):\n",
    "   # Get the new summary.\n",
    "   new_summary = chain.invoke({\"text\": doc.page_content})\n",
    "   # Update the list of the last two summaries: remove the first one and add the new one at the end.\n",
    "   final_summary+=new_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mibey\\AppData\\Local\\Temp\\ipykernel_26548\\1606759991.py:6: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  self.set_font('Arial', 'B', 15)\n",
      "C:\\Users\\mibey\\AppData\\Local\\Temp\\ipykernel_26548\\1606759991.py:10: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  self.cell(30, 10, 'Summary', 1, 0, 'C')\n",
      "C:\\Users\\mibey\\AppData\\Local\\Temp\\ipykernel_26548\\1606759991.py:25: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "C:\\Users\\mibey\\AppData\\Local\\Temp\\ipykernel_26548\\1606759991.py:18: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  self.set_font('Arial', 'I', 8)\n",
      "C:\\Users\\mibey\\AppData\\Local\\Temp\\ipykernel_26548\\1606759991.py:20: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=0 use new_x=XPos.RIGHT, new_y=YPos.TOP.\n",
      "  self.cell(0, 10, 'Page %s' % self.page_no(), 0, 0, 'C')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "   def header(self):\n",
    "       # Select Arial bold 15\n",
    "       self.set_font('Arial', 'B', 15)\n",
    "       # Move to the right\n",
    "       self.cell(80)\n",
    "       # Framed title\n",
    "       self.cell(30, 10, 'Summary', 1, 0, 'C')\n",
    "       # Line break\n",
    "       self.ln(20)\n",
    "\n",
    "   def footer(self):\n",
    "       # Go to 1.5 cm from bottom\n",
    "       self.set_y(-15)\n",
    "       # Select Arial italic 8\n",
    "       self.set_font('Arial', 'I', 8)\n",
    "       # Page number\n",
    "       self.cell(0, 10, 'Page %s' % self.page_no(), 0, 0, 'C')\n",
    "\n",
    "# Instantiate PDF object and add a page\n",
    "pdf = PDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "# Ensure the 'last_summary' text is treated as UTF-8\n",
    "# Replace 'last_summary' with your actual text variable if different\n",
    "# Make sure your text is a utf-8 encoded string\n",
    "last_summary_utf8 = final_summary.encode('latin-1', 'replace').decode('latin-1')\n",
    "pdf.multi_cell(0, 10, last_summary_utf8)\n",
    "\n",
    "# Save the PDF to a file\n",
    "pdf_output_path = \"s_output1.pdf\"\n",
    "pdf.output(pdf_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
